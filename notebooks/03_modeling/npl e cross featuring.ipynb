{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-26T00:14:09.701418Z",
     "start_time": "2026-01-26T00:13:51.429599Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# ======================================================\n",
    "# 1Ô∏è‚É£ PATH CORRETTI\n",
    "# ======================================================\n",
    "\n",
    "BASE_IN  = \"/Users/marcodonatiello/PycharmProjects/JupyterProject/data/interim/npl\"\n",
    "BASE_OUT = \"/Users/marcodonatiello/PycharmProjects/JupyterProject/dataset\"\n",
    "\n",
    "TRAIN_PATH = f\"{BASE_IN}/train.csv\"\n",
    "VAL_PATH   = f\"{BASE_IN}/val.csv\"\n",
    "TEST_PATH  = f\"{BASE_IN}/test.csv\"\n",
    "\n",
    "OUT_TRAIN = f\"{BASE_OUT}/train_step5.csv\"\n",
    "OUT_VAL   = f\"{BASE_OUT}/val_step5.csv\"\n",
    "OUT_TEST  = f\"{BASE_OUT}/test_step5.csv\"\n",
    "\n",
    "# ======================================================\n",
    "# 2Ô∏è‚É£ LOAD DATASET\n",
    "# ======================================================\n",
    "\n",
    "for p in [TRAIN_PATH, VAL_PATH, TEST_PATH]:\n",
    "    assert os.path.exists(p), f\"‚ùå File non trovato: {p}\"\n",
    "\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "val   = pd.read_csv(VAL_PATH)\n",
    "test  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(\"Train:\", train.shape)\n",
    "print(\"Val:\", val.shape)\n",
    "print(\"Test:\", test.shape)\n",
    "\n",
    "# ======================================================\n",
    "# 3Ô∏è‚É£ DROP VAERS_ID (CONSENTITO SU TUTTI)\n",
    "# ======================================================\n",
    "\n",
    "for df in [train, val, test]:\n",
    "    if \"VAERS_ID\" in df.columns:\n",
    "        df.drop(columns=[\"VAERS_ID\"], inplace=True)\n",
    "\n",
    "# ======================================================\n",
    "# 4Ô∏è‚É£ NLP / NEL ‚Äî SOLO TRAIN E VAL\n",
    "# ======================================================\n",
    "\n",
    "def clean_symptom_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s;]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "SYMPTOM_MAP = {\n",
    "    \"cardiac\": [\n",
    "        \"chest pain\",\"myocarditis\",\"pericarditis\",\n",
    "        \"tachycardia\",\"palpitations\",\"cardiac arrest\",\"arrhythmia\"\n",
    "    ],\n",
    "    \"respiratory\": [\n",
    "        \"dyspnoea\",\"shortness of breath\",\"respiratory distress\",\n",
    "        \"cough\",\"hypoxia\",\"pneumonia\"\n",
    "    ],\n",
    "    \"neurologic\": [\n",
    "        \"headache\",\"seizure\",\"syncope\",\"loss of consciousness\",\n",
    "        \"dizziness\",\"stroke\",\"paresthesia\"\n",
    "    ],\n",
    "    \"fever\": [\n",
    "        \"fever\",\"pyrexia\",\"chills\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def extract_symptoms(text):\n",
    "    feats = {}\n",
    "    for group, kws in SYMPTOM_MAP.items():\n",
    "        count = sum(\n",
    "            1 for kw in kws if re.search(rf\"\\b{re.escape(kw)}\\b\", text)\n",
    "        )\n",
    "        feats[f\"symp_{group}\"] = int(count > 0)\n",
    "        feats[f\"num_symp_{group}\"] = count\n",
    "\n",
    "    feats[\"num_symp_total\"] = sum(\n",
    "        v for k, v in feats.items() if k.startswith(\"num_symp_\")\n",
    "    )\n",
    "    return feats\n",
    "\n",
    "def apply_nel(df):\n",
    "    clean_text = df[\"LISTA_SINTOMI\"].apply(clean_symptom_text)\n",
    "\n",
    "    feats = []\n",
    "    for text in tqdm(clean_text, desc=\"NLP/NEL\"):\n",
    "        feats.append(extract_symptoms(text))\n",
    "\n",
    "    feats_df = pd.DataFrame(feats)\n",
    "    return pd.concat([df.reset_index(drop=True), feats_df], axis=1)\n",
    "\n",
    "train = apply_nel(train)\n",
    "val   = apply_nel(val)\n",
    "\n",
    "print(\"‚úÖ NLP/NEL applicato a train e val\")\n",
    "\n",
    "# ======================================================\n",
    "# 5Ô∏è‚É£ FEATURE CROSSING ‚Äî SOLO TRAIN E VAL\n",
    "# ======================================================\n",
    "# 1) AGE_YRS √ó NUMERO_SINTOMI\n",
    "# 2) has_history √ó NUMERO_SINTOMI\n",
    "# 3) AGE_YRS √ó history_cardiac\n",
    "\n",
    "def add_feature_crossing(df):\n",
    "    df[\"fc_age_x_num_symptoms\"] = df[\"AGE_YRS\"] * df[\"NUMERO_SINTOMI\"]\n",
    "    df[\"fc_history_x_num_symptoms\"] = df[\"has_history\"] * df[\"NUMERO_SINTOMI\"]\n",
    "    df[\"fc_age_x_history_cardiac\"] = df[\"AGE_YRS\"] * df[\"history_cardiac\"]\n",
    "    return df\n",
    "\n",
    "train = add_feature_crossing(train)\n",
    "val   = add_feature_crossing(val)\n",
    "\n",
    "print(\"‚úÖ Feature crossing aggiunte (train + val)\")\n",
    "\n",
    "# ======================================================\n",
    "# 6Ô∏è‚É£ DROP LISTA_SINTOMI TESTUALE (TUTTI)\n",
    "# ======================================================\n",
    "\n",
    "for df in [train, val, test]:\n",
    "    if \"LISTA_SINTOMI\" in df.columns:\n",
    "        df.drop(columns=[\"LISTA_SINTOMI\"], inplace=True)\n",
    "\n",
    "print(\"‚úÖ LISTA_SINTOMI testuale rimossa da tutti\")\n",
    "\n",
    "# ======================================================\n",
    "# 7Ô∏è‚É£ SALVATAGGIO\n",
    "# ======================================================\n",
    "\n",
    "train.to_csv(OUT_TRAIN, index=False)\n",
    "val.to_csv(OUT_VAL, index=False)\n",
    "test.to_csv(OUT_TEST, index=False)\n",
    "\n",
    "print(\"\\nüöÄ PIPELINE COMPLETATA CON SUCCESSO\")\n",
    "print(\"Train ‚Üí\", OUT_TRAIN)\n",
    "print(\"Val   ‚Üí\", OUT_VAL)\n",
    "print(\"Test  ‚Üí\", OUT_TEST)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (536370, 68)\n",
      "Val: (134093, 68)\n",
      "Test: (167616, 68)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLP/NEL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 536370/536370 [00:09<00:00, 59107.30it/s]\n",
      "NLP/NEL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 134093/134093 [00:02<00:00, 59364.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NLP/NEL applicato a train e val\n",
      "‚úÖ Feature crossing aggiunte (train + val)\n",
      "‚úÖ LISTA_SINTOMI testuale rimossa da tutti\n",
      "\n",
      "üöÄ PIPELINE COMPLETATA CON SUCCESSO\n",
      "Train ‚Üí /Users/marcodonatiello/PycharmProjects/JupyterProject/data/interim/splits/train_step5.csv\n",
      "Val   ‚Üí /Users/marcodonatiello/PycharmProjects/JupyterProject/data/interim/splits/val_step5.csv\n",
      "Test  ‚Üí /Users/marcodonatiello/PycharmProjects/JupyterProject/data/interim/splits/test_step5.csv\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
