{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-30T18:58:36.799206Z",
     "start_time": "2026-01-30T18:56:09.833594Z"
    }
   },
   "source": [
    "# ======================================================\n",
    "# STAGE 2 ‚Äî STACKING + THRESHOLD TUNING + FN ANALYSIS\n",
    "# ======================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ======================================================\n",
    "# 1Ô∏è‚É£ PATH & PARAMS\n",
    "# ======================================================\n",
    "\n",
    "TRAIN_PATH = \"/Users/marcodonatiello/PycharmProjects/JupyterProject/data/interim/splits/train_step6.csv\"\n",
    "VAL_PATH   = \"/Users/marcodonatiello/PycharmProjects/JupyterProject/data/interim/splits/val_step6.csv\"\n",
    "\n",
    "TARGET = \"IS_SEVERE\"\n",
    "N_SPLITS = 5\n",
    "\n",
    "# soglie da testare\n",
    "THRESHOLDS = np.arange(0.20, 0.61, 0.02)\n",
    "\n",
    "# ======================================================\n",
    "# 2Ô∏è‚É£ LOAD DATA\n",
    "# ======================================================\n",
    "\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "val   = pd.read_csv(VAL_PATH)\n",
    "\n",
    "X_train = train.drop(columns=[TARGET])\n",
    "y_train = train[TARGET]\n",
    "\n",
    "X_val = val.drop(columns=[TARGET])\n",
    "y_val = val[TARGET]\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Val shape  :\", X_val.shape)\n",
    "\n",
    "# ======================================================\n",
    "# 3Ô∏è‚É£ CLASS IMBALANCE\n",
    "# ======================================================\n",
    "\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(\"scale_pos_weight:\", scale_pos_weight)\n",
    "\n",
    "# ======================================================\n",
    "# 4Ô∏è‚É£ BASE MODELS\n",
    "# ======================================================\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=15,\n",
    "    min_samples_leaf=10,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    eval_metric=\"logloss\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lgbm = LGBMClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=-1,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ======================================================\n",
    "# 5Ô∏è‚É£ OOF META-FEATURES (TRAIN)\n",
    "# ======================================================\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "train_meta = pd.DataFrame(\n",
    "    np.zeros((len(train), 3)),\n",
    "    columns=[\"rf_prob\", \"xgb_prob\", \"lgbm_prob\"]\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Generating OOF meta-features...\")\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
    "    print(f\"Fold {fold}\")\n",
    "\n",
    "    rf.fit(X_train.iloc[tr_idx], y_train.iloc[tr_idx])\n",
    "    xgb.fit(X_train.iloc[tr_idx], y_train.iloc[tr_idx])\n",
    "    lgbm.fit(X_train.iloc[tr_idx], y_train.iloc[tr_idx])\n",
    "\n",
    "    train_meta.iloc[val_idx, 0] = rf.predict_proba(X_train.iloc[val_idx])[:, 1]\n",
    "    train_meta.iloc[val_idx, 1] = xgb.predict_proba(X_train.iloc[val_idx])[:, 1]\n",
    "    train_meta.iloc[val_idx, 2] = lgbm.predict_proba(X_train.iloc[val_idx])[:, 1]\n",
    "\n",
    "print(\"‚úÖ OOF meta-features generated\")\n",
    "\n",
    "# ======================================================\n",
    "# 6Ô∏è‚É£ TRAIN BASE MODELS ON FULL TRAIN\n",
    "# ======================================================\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "xgb.fit(X_train, y_train)\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "val_meta = pd.DataFrame({\n",
    "    \"rf_prob\": rf.predict_proba(X_val)[:, 1],\n",
    "    \"xgb_prob\": xgb.predict_proba(X_val)[:, 1],\n",
    "    \"lgbm_prob\": lgbm.predict_proba(X_val)[:, 1]\n",
    "})\n",
    "\n",
    "# ======================================================\n",
    "# 7Ô∏è‚É£ META-MODEL\n",
    "# ======================================================\n",
    "\n",
    "meta_model = LogisticRegression(\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Training meta-model...\")\n",
    "meta_model.fit(train_meta, y_train)\n",
    "print(\"‚úÖ Meta-model trained\")\n",
    "\n",
    "val_probs = meta_model.predict_proba(val_meta)[:, 1]\n",
    "\n",
    "# ======================================================\n",
    "# 8Ô∏è‚É£ THRESHOLD TUNING (VALIDATION)\n",
    "# ======================================================\n",
    "\n",
    "rows = []\n",
    "for t in THRESHOLDS:\n",
    "    pred = (val_probs >= t).astype(int)\n",
    "\n",
    "    rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"recall\": recall_score(y_val, pred),\n",
    "        \"precision\": precision_score(y_val, pred),\n",
    "        \"f1\": f1_score(y_val, pred),\n",
    "        \"false_negatives\": ((y_val == 1) & (pred == 0)).sum(),\n",
    "        \"false_positives\": ((y_val == 0) & (pred == 1)).sum()\n",
    "    })\n",
    "\n",
    "thr_df = pd.DataFrame(rows).sort_values(\"f1\", ascending=False)\n",
    "\n",
    "print(\"\\nüìä THRESHOLD TUNING RESULTS (TOP 10)\")\n",
    "print(thr_df.head(10))\n",
    "\n",
    "BEST_T = thr_df.iloc[0][\"threshold\"]\n",
    "print(\"\\nüèÜ BEST THRESHOLD:\", BEST_T)\n",
    "\n",
    "# ======================================================\n",
    "# 9Ô∏è‚É£ FINAL EVALUATION (BEST THRESHOLD)\n",
    "# ======================================================\n",
    "\n",
    "val_pred = (val_probs >= BEST_T).astype(int)\n",
    "\n",
    "print(\"\\nüìä CLASSIFICATION REPORT (FINAL)\")\n",
    "print(classification_report(y_val, val_pred, digits=4))\n",
    "\n",
    "print(\"üìâ CONFUSION MATRIX\")\n",
    "print(confusion_matrix(y_val, val_pred))\n",
    "\n",
    "# ======================================================\n",
    "# üîü FN ANALYSIS POST-FEATURE\n",
    "# ======================================================\n",
    "\n",
    "val_analysis = val.copy()\n",
    "val_analysis[\"prob\"] = val_probs\n",
    "val_analysis[\"pred\"] = val_pred\n",
    "\n",
    "fn = val_analysis[(y_val == 1) & (val_pred == 0)]\n",
    "tp = val_analysis[(y_val == 1) & (val_pred == 1)]\n",
    "\n",
    "print(\"\\nüìâ FALSE NEGATIVES:\", fn.shape[0])\n",
    "print(\"‚úÖ TRUE POSITIVES :\", tp.shape[0])\n",
    "\n",
    "numeric_cols = val.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "numeric_cols = [c for c in numeric_cols if c not in [TARGET, \"prob\", \"pred\"]]\n",
    "\n",
    "rows = []\n",
    "for col in numeric_cols:\n",
    "    rows.append({\n",
    "        \"feature\": col,\n",
    "        \"FN_mean\": fn[col].mean(),\n",
    "        \"TP_mean\": tp[col].mean(),\n",
    "        \"delta_FN_minus_TP\": fn[col].mean() - tp[col].mean()\n",
    "    })\n",
    "\n",
    "diff_df = (\n",
    "    pd.DataFrame(rows)\n",
    "    .sort_values(by=\"delta_FN_minus_TP\", key=abs, ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\nüîç TOP 15 DIFFERENZE FN vs TP\")\n",
    "print(diff_df.head(15))\n",
    "\n",
    "fn[\"prob_bucket\"] = pd.cut(\n",
    "    fn[\"prob\"],\n",
    "    bins=[0, 0.2, 0.4, 0.6, 1.0],\n",
    "    labels=[\"very_low\", \"low\", \"borderline\", \"high\"]\n",
    ")\n",
    "\n",
    "print(\"\\nüìä FN ‚Äî DISTRIBUZIONE PROBABILIT√Ä\")\n",
    "print(fn[\"prob_bucket\"].value_counts(normalize=True))\n",
    "\n",
    "# ======================================================\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ SAVE FILES\n",
    "# ======================================================\n",
    "\n",
    "fn.to_csv(\"false_negatives_stage2.csv\", index=False)\n",
    "tp.sample(2000, random_state=42).to_csv(\"true_positives_stage2_sample.csv\", index=False)\n",
    "\n",
    "print(\"\\nüìÅ FILE SALVATI\")\n",
    "print(\"- false_negatives_stage2.csv\")\n",
    "print(\"- true_positives_stage2_sample.csv\")\n",
    "\n",
    "print(\"\\nüèÜ STAGE 2 COMPLETATO ‚Äî TEST SET NON UTILIZZATO\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (536370, 78)\n",
      "Val shape  : (134093, 78)\n",
      "scale_pos_weight: 6.786794808513109\n",
      "\n",
      "üöÄ Generating OOF meta-features...\n",
      "Fold 1\n",
      "[LightGBM] [Info] Number of positive: 55106, number of negative: 373990\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019690 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1046\n",
      "[LightGBM] [Info] Number of data points in the train set: 429096, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Fold 2\n",
      "[LightGBM] [Info] Number of positive: 55106, number of negative: 373990\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017645 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1046\n",
      "[LightGBM] [Info] Number of data points in the train set: 429096, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Fold 3\n",
      "[LightGBM] [Info] Number of positive: 55106, number of negative: 373990\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1037\n",
      "[LightGBM] [Info] Number of data points in the train set: 429096, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Fold 4\n",
      "[LightGBM] [Info] Number of positive: 55105, number of negative: 373991\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1053\n",
      "[LightGBM] [Info] Number of data points in the train set: 429096, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Fold 5\n",
      "[LightGBM] [Info] Number of positive: 55105, number of negative: 373991\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030071 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1045\n",
      "[LightGBM] [Info] Number of data points in the train set: 429096, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "‚úÖ OOF meta-features generated\n",
      "[LightGBM] [Info] Number of positive: 68882, number of negative: 467488\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1042\n",
      "[LightGBM] [Info] Number of data points in the train set: 536370, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "\n",
      "üöÄ Training meta-model...\n",
      "‚úÖ Meta-model trained\n",
      "\n",
      "üìä THRESHOLD TUNING RESULTS (TOP 10)\n",
      "    threshold    recall  precision        f1  false_negatives  false_positives\n",
      "20       0.60  0.791417   0.464741  0.585602             3592            15697\n",
      "19       0.58  0.799083   0.454563  0.579484             3460            16512\n",
      "18       0.56  0.808548   0.444799  0.573890             3297            17380\n",
      "17       0.54  0.816329   0.435300  0.567816             3163            18237\n",
      "16       0.52  0.824517   0.425923  0.561692             3022            19138\n",
      "15       0.50  0.832007   0.417580  0.556071             2893            19984\n",
      "14       0.48  0.838279   0.409218  0.549964             2785            20841\n",
      "13       0.46  0.845421   0.401373  0.544323             2662            21714\n",
      "12       0.44  0.851170   0.393176  0.537889             2563            22623\n",
      "11       0.42  0.857209   0.385109  0.531456             2459            23570\n",
      "\n",
      "üèÜ BEST THRESHOLD: 0.5999999999999999\n",
      "\n",
      "üìä CLASSIFICATION REPORT (FINAL)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9657    0.8657    0.9130    116872\n",
      "           1     0.4647    0.7914    0.5856     17221\n",
      "\n",
      "    accuracy                         0.8562    134093\n",
      "   macro avg     0.7152    0.8286    0.7493    134093\n",
      "weighted avg     0.9014    0.8562    0.8709    134093\n",
      "\n",
      "üìâ CONFUSION MATRIX\n",
      "[[101175  15697]\n",
      " [  3592  13629]]\n",
      "\n",
      "üìâ FALSE NEGATIVES: 3592\n",
      "‚úÖ TRUE POSITIVES : 13629\n",
      "\n",
      "üîç TOP 15 DIFFERENZE FN vs TP\n",
      "                      feature     FN_mean     TP_mean  delta_FN_minus_TP\n",
      "74      fc_age_x_num_symptoms  234.294822  514.581627        -280.286806\n",
      "1                     NUMDAYS   14.590200  151.736811        -137.146611\n",
      "0                     AGE_YRS   51.557350   65.631301         -14.073951\n",
      "3              NUMERO_SINTOMI    4.661470    8.262382          -3.600912\n",
      "75  fc_history_x_num_symptoms    4.661470    8.262382          -3.600912\n",
      "76   fc_age_x_history_cardiac    0.125278    0.828087          -0.702809\n",
      "68       num_symp_respiratory    0.114143    0.497542          -0.383399\n",
      "2             VAX_DOSE_SERIES    1.611080    1.850026          -0.238946\n",
      "73             num_symp_total    0.693486    0.920464          -0.226978\n",
      "4                       SEX_F    0.691815    0.465038           0.226777\n",
      "5                       SEX_M    0.308185    0.534962          -0.226777\n",
      "67           symp_respiratory    0.103563    0.314550          -0.210986\n",
      "77     ratio_symp_respiratory    0.061974    0.253043          -0.191069\n",
      "70        num_symp_neurologic    0.293987    0.140436           0.153551\n",
      "69            symp_neurologic    0.246938    0.118204           0.128734\n",
      "\n",
      "üìä FN ‚Äî DISTRIBUZIONE PROBABILIT√Ä\n",
      "prob_bucket\n",
      "borderline    0.347996\n",
      "very_low      0.345768\n",
      "low           0.306236\n",
      "high          0.000000\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "üìÅ FILE SALVATI\n",
      "- false_negatives_stage2.csv\n",
      "- true_positives_stage2_sample.csv\n",
      "\n",
      "üèÜ STAGE 2 COMPLETATO ‚Äî TEST SET NON UTILIZZATO\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T19:07:10.506919Z",
     "start_time": "2026-01-30T19:07:10.460931Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "bbe9d02d0edd0d5a",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_pred_proba' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# =========================\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# SOGLIA BASE\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[38;5;66;03m# =========================\u001B[39;00m\n\u001B[32m      7\u001B[39m THRESHOLD = \u001B[32m0.60\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m y_pred_base = (\u001B[43mval_pred_proba\u001B[49m >= THRESHOLD).astype(\u001B[38;5;28mint\u001B[39m)\n\u001B[32m     10\u001B[39m \u001B[38;5;66;03m# =========================\u001B[39;00m\n\u001B[32m     11\u001B[39m \u001B[38;5;66;03m# SAFETY NET (semplice)\u001B[39;00m\n\u001B[32m     12\u001B[39m \u001B[38;5;66;03m# =========================\u001B[39;00m\n\u001B[32m     13\u001B[39m safety_mask = (\n\u001B[32m     14\u001B[39m     (val_pred_proba >= \u001B[32m0.45\u001B[39m) &\n\u001B[32m     15\u001B[39m     (val_pred_proba < THRESHOLD) &\n\u001B[32m     16\u001B[39m     (X_val[\u001B[33m\"\u001B[39m\u001B[33mratio_symp_respiratory\u001B[39m\u001B[33m\"\u001B[39m] >= \u001B[32m0.30\u001B[39m) &\n\u001B[32m     17\u001B[39m     (X_val[\u001B[33m\"\u001B[39m\u001B[33mAGE_YRS\u001B[39m\u001B[33m\"\u001B[39m] >= \u001B[32m50\u001B[39m)\n\u001B[32m     18\u001B[39m )\n",
      "\u001B[31mNameError\u001B[39m: name 'val_pred_proba' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T13:42:22.533375Z",
     "start_time": "2026-02-13T13:42:22.073097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ======================================================\n",
    "# STAGE 2 ‚Äî STACKING + THRESHOLD TUNING + FN ANALYSIS\n",
    "# (Versione per Dataset SMOTE Bilanciato)\n",
    "# ======================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ======================================================\n",
    "# 1Ô∏è‚É£ PATH & PARAMS\n",
    "# ======================================================\n",
    "\n",
    "# ‚ö†Ô∏è MODIFICA QUI: Puntiamo al file SMOTE creato prima\n",
    "# Assumiamo siano nella cartella corrente. Se sono altrove, rimetti il percorso completo.\n",
    "TRAIN_PATH = \"train_step6_SMOTE.csv\"\n",
    "VAL_PATH   = \"val_step6.csv\"          # Il validation deve restare quello ORIGINALE\n",
    "\n",
    "TARGET = \"IS_SEVERE\"\n",
    "N_SPLITS = 5\n",
    "\n",
    "# Range di soglie (Thresholds)\n",
    "THRESHOLDS = np.arange(0.20, 0.81, 0.05) # Ho esteso un po' il range verso l'alto\n",
    "\n",
    "# ======================================================\n",
    "# 2Ô∏è‚É£ LOAD DATA\n",
    "# ======================================================\n",
    "\n",
    "if not os.path.exists(TRAIN_PATH):\n",
    "    print(f\"‚ùå Errore: Non trovo {TRAIN_PATH}. Assicurati di aver eseguito lo script di salvataggio prima.\")\n",
    "else:\n",
    "    print(f\"üìÇ Caricamento Train (SMOTE): {TRAIN_PATH}\")\n",
    "    train = pd.read_csv(TRAIN_PATH)\n",
    "\n",
    "print(f\"üìÇ Caricamento Val (Originale): {VAL_PATH}\")\n",
    "val = pd.read_csv(VAL_PATH)\n",
    "\n",
    "X_train = train.drop(columns=[TARGET])\n",
    "y_train = train[TARGET]\n",
    "\n",
    "X_val = val.drop(columns=[TARGET])\n",
    "y_val = val[TARGET]\n",
    "\n",
    "print(\"Train shape (Balanced):\", X_train.shape)\n",
    "print(\"Val shape (Imbalanced):\", X_val.shape)\n",
    "\n",
    "# ======================================================\n",
    "# 3Ô∏è‚É£ BILANCIAMENTO (Ora √® 1:1)\n",
    "# ======================================================\n",
    "\n",
    "# Dato che abbiamo usato SMOTE, il rapporto √® circa 1.\n",
    "# Non forziamo pi√π scale_pos_weight.\n",
    "scale_pos_weight = 1.0\n",
    "print(\"scale_pos_weight impostato a 1.0 (Dati gi√† bilanciati da SMOTE)\")\n",
    "\n",
    "# ======================================================\n",
    "# 4Ô∏è‚É£ BASE MODELS (Senza Class Weights)\n",
    "# ======================================================\n",
    "\n",
    "# ‚ö†Ô∏è NOTA: Ho rimosso 'class_weight=\"balanced\"' perch√© SMOTE ha gi√† fatto il lavoro.\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=15,\n",
    "    min_samples_leaf=10,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    "    # class_weight=\"balanced\"  <-- RIMOSSO\n",
    ")\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=1,        # <-- IMPOSTATO A 1\n",
    "    eval_metric=\"logloss\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lgbm = LGBMClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=-1,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    # class_weight=\"balanced\", <-- RIMOSSO\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# ======================================================\n",
    "# 5Ô∏è‚É£ OOF META-FEATURES (TRAIN)\n",
    "# ======================================================\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "# Matrice vuota per salvare le previsioni \"Out Of Fold\"\n",
    "train_meta = pd.DataFrame(\n",
    "    np.zeros((len(train), 3)),\n",
    "    columns=[\"rf_prob\", \"xgb_prob\", \"lgbm_prob\"]\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Generating OOF meta-features (Stacking)...\")\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
    "    print(f\"  -> Fold {fold}/{N_SPLITS} processing...\")\n",
    "\n",
    "    # Dati del fold corrente\n",
    "    X_tr_fold, y_tr_fold = X_train.iloc[tr_idx], y_train.iloc[tr_idx]\n",
    "    X_val_fold = X_train.iloc[val_idx]\n",
    "\n",
    "    # Addestramento modelli base sul fold\n",
    "    rf.fit(X_tr_fold, y_tr_fold)\n",
    "    xgb.fit(X_tr_fold, y_tr_fold)\n",
    "    lgbm.fit(X_tr_fold, y_tr_fold)\n",
    "\n",
    "    # Predizione sulla parte \"lasciata fuori\" (Hold-out)\n",
    "    train_meta.iloc[val_idx, 0] = rf.predict_proba(X_val_fold)[:, 1]\n",
    "    train_meta.iloc[val_idx, 1] = xgb.predict_proba(X_val_fold)[:, 1]\n",
    "    train_meta.iloc[val_idx, 2] = lgbm.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "print(\"‚úÖ OOF meta-features generated\")\n",
    "\n",
    "# ======================================================\n",
    "# 6Ô∏è‚É£ TRAIN BASE MODELS ON FULL TRAIN\n",
    "# ======================================================\n",
    "print(\"\\nüöÄ Re-training base models on FULL Train set...\")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "xgb.fit(X_train, y_train)\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "# Generiamo le feature per il validation set reale\n",
    "val_meta = pd.DataFrame({\n",
    "    \"rf_prob\": rf.predict_proba(X_val)[:, 1],\n",
    "    \"xgb_prob\": xgb.predict_proba(X_val)[:, 1],\n",
    "    \"lgbm_prob\": lgbm.predict_proba(X_val)[:, 1]\n",
    "})\n",
    "\n",
    "# ======================================================\n",
    "# 7Ô∏è‚É£ META-MODEL (Logistic Regression)\n",
    "# ======================================================\n",
    "\n",
    "meta_model = LogisticRegression(\n",
    "    # Qui possiamo lasciare balanced o toglierlo, ma su meta-features\n",
    "    # spesso √® meglio lasciarlo neutro se il train era SMOTE.\n",
    "    # Proviamo neutro per vedere la pura probabilit√†.\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"üöÄ Training Meta-Model (The Judge)...\")\n",
    "meta_model.fit(train_meta, y_train)\n",
    "print(\"‚úÖ Meta-model trained\")\n",
    "\n",
    "# Predizioni finali (Probabilit√† combinate)\n",
    "val_probs = meta_model.predict_proba(val_meta)[:, 1]\n",
    "\n",
    "# ======================================================\n",
    "# 8Ô∏è‚É£ THRESHOLD TUNING (VALIDATION)\n",
    "# ======================================================\n",
    "\n",
    "rows = []\n",
    "print(\"\\nüîé Testing Thresholds...\")\n",
    "for t in THRESHOLDS:\n",
    "    pred = (val_probs >= t).astype(int)\n",
    "\n",
    "    rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"recall\": recall_score(y_val, pred),\n",
    "        \"precision\": precision_score(y_val, pred),\n",
    "        \"f1\": f1_score(y_val, pred),\n",
    "        \"false_negatives\": ((y_val == 1) & (pred == 0)).sum(),\n",
    "        \"false_positives\": ((y_val == 0) & (pred == 1)).sum()\n",
    "    })\n",
    "\n",
    "thr_df = pd.DataFrame(rows).sort_values(\"f1\", ascending=False)\n",
    "\n",
    "print(\"\\nüìä THRESHOLD TUNING RESULTS (TOP 5 per F1)\")\n",
    "print(thr_df.head(5))\n",
    "\n",
    "# Scegliamo la soglia migliore basata su F1 (puoi cambiare logica se preferisci Recall)\n",
    "BEST_T = thr_df.iloc[0][\"threshold\"]\n",
    "print(f\"\\nüèÜ BEST THRESHOLD: {BEST_T:.2f}\")\n",
    "\n",
    "# ======================================================\n",
    "# 9Ô∏è‚É£ FINAL EVALUATION (BEST THRESHOLD)\n",
    "# ======================================================\n",
    "\n",
    "val_pred = (val_probs >= BEST_T).astype(int)\n",
    "\n",
    "print(\"\\nüìä CLASSIFICATION REPORT (FINAL STACKING)\")\n",
    "print(classification_report(y_val, val_pred, digits=4))\n",
    "\n",
    "print(\"üìâ CONFUSION MATRIX\")\n",
    "print(confusion_matrix(y_val, val_pred))\n",
    "\n",
    "# ======================================================\n",
    "# üîü FN ANALYSIS\n",
    "# ======================================================\n",
    "\n",
    "val_analysis = val.copy()\n",
    "val_analysis[\"prob\"] = val_probs\n",
    "val_analysis[\"pred\"] = val_pred\n",
    "\n",
    "fn = val_analysis[(y_val == 1) & (val_pred == 0)] # Malati persi\n",
    "tp = val_analysis[(y_val == 1) & (val_pred == 1)] # Malati presi\n",
    "\n",
    "print(f\"\\nüìâ FALSE NEGATIVES: {fn.shape[0]}\")\n",
    "print(f\"‚úÖ TRUE POSITIVES : {tp.shape[0]}\")\n",
    "\n",
    "# Salvataggio per analisi manuale\n",
    "fn.to_csv(\"false_negatives_stacking.csv\", index=False)\n",
    "print(\"\\nüìÅ Analisi salvata in 'false_negatives_stacking.csv'\")"
   ],
   "id": "db78dd52a7e11eda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Errore: Non trovo train_step6_SMOTE.csv. Assicurati di aver eseguito lo script di salvataggio prima.\n",
      "üìÇ Caricamento Val (Originale): val_step6.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'val_step6.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 50\u001B[39m\n\u001B[32m     47\u001B[39m     train = pd.read_csv(TRAIN_PATH)\n\u001B[32m     49\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33müìÇ Caricamento Val (Originale): \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mVAL_PATH\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m50\u001B[39m val = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mVAL_PATH\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     52\u001B[39m X_train = train.drop(columns=[TARGET])\n\u001B[32m     53\u001B[39m y_train = train[TARGET]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/JupyterProject/.venv/lib/python3.14/site-packages/pandas/io/parsers/readers.py:873\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m    861\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m    862\u001B[39m     dialect,\n\u001B[32m    863\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m    869\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m    870\u001B[39m )\n\u001B[32m    871\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m--> \u001B[39m\u001B[32m873\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/JupyterProject/.venv/lib/python3.14/site-packages/pandas/io/parsers/readers.py:300\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    297\u001B[39m _validate_names(kwds.get(\u001B[33m\"\u001B[39m\u001B[33mnames\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    299\u001B[39m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m300\u001B[39m parser = \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    302\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[32m    303\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/JupyterProject/.venv/lib/python3.14/site-packages/pandas/io/parsers/readers.py:1645\u001B[39m, in \u001B[36mTextFileReader.__init__\u001B[39m\u001B[34m(self, f, engine, **kwds)\u001B[39m\n\u001B[32m   1642\u001B[39m     \u001B[38;5;28mself\u001B[39m.options[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m] = kwds[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1644\u001B[39m \u001B[38;5;28mself\u001B[39m.handles: IOHandles | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1645\u001B[39m \u001B[38;5;28mself\u001B[39m._engine = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/JupyterProject/.venv/lib/python3.14/site-packages/pandas/io/parsers/readers.py:1904\u001B[39m, in \u001B[36mTextFileReader._make_engine\u001B[39m\u001B[34m(self, f, engine)\u001B[39m\n\u001B[32m   1902\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[32m   1903\u001B[39m         mode += \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1904\u001B[39m \u001B[38;5;28mself\u001B[39m.handles = \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1905\u001B[39m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1906\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1907\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1908\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcompression\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1909\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmemory_map\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1910\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1911\u001B[39m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding_errors\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstrict\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1912\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstorage_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1913\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1914\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1915\u001B[39m f = \u001B[38;5;28mself\u001B[39m.handles.handle\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/JupyterProject/.venv/lib/python3.14/site-packages/pandas/io/common.py:926\u001B[39m, in \u001B[36mget_handle\u001B[39m\u001B[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[39m\n\u001B[32m    921\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    922\u001B[39m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[32m    923\u001B[39m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[32m    924\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m ioargs.encoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs.mode:\n\u001B[32m    925\u001B[39m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m926\u001B[39m         handle = \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    928\u001B[39m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    929\u001B[39m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    931\u001B[39m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    933\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    934\u001B[39m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[32m    935\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(handle, ioargs.mode)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'val_step6.csv'"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
