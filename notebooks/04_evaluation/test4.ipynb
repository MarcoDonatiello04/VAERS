{
 "cells": [
  {
   "cell_type": "code",
   "id": "17ea2ca5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:18:50.744865Z",
     "start_time": "2026-02-13T19:18:34.827591Z"
    }
   },
   "source": [
    "# ======================================================\n",
    "# TEST4 - BEST TACTIC (TUNED WEIGHTED LIGHTGBM)\n",
    "# Objective: maximize class-1 metrics with precision-focused thresholding\n",
    "# ======================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 1) CONFIG\n",
    "# ======================================================\n",
    "\n",
    "TARGET = \"IS_SEVERE\"\n",
    "MIN_RECALL = 0.58                      # trade-off scelto da benchmark\n",
    "THRESHOLDS = np.round(np.arange(0.10, 0.951, 0.01), 2)\n",
    "RANDOM_STATE = 42\n",
    "SAVE_MODEL = True\n",
    "\n",
    "# Parametri tuned emersi dal benchmark\n",
    "LGBM_PARAMS = {\n",
    "    \"n_estimators\": 650,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"num_leaves\": 95,\n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"min_child_samples\": 60,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 3.0,\n",
    "}\n",
    "\n",
    "\n",
    "def find_project_root() -> Path:\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for p in [cwd, *cwd.parents]:\n",
    "        if (p / \"data\").exists() and (p / \"models\").exists():\n",
    "            return p\n",
    "    return cwd\n",
    "\n",
    "\n",
    "ROOT = find_project_root()\n",
    "print(f\"Project root: {ROOT}\")\n",
    "\n",
    "TRAIN_PATH = ROOT / \"data\" / \"interim\" / \"splits\" / \"train_step6.csv\"\n",
    "VAL_PATH = ROOT / \"data\" / \"interim\" / \"splits\" / \"val_step6.csv\"\n",
    "TEST_PATH = ROOT / \"data\" / \"interim\" / \"splits\" / \"test_step6.csv\"\n",
    "\n",
    "REPORTS_DIR = ROOT / \"reports\" / \"metrics\"\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 2) HELPERS\n",
    "# ======================================================\n",
    "\n",
    "def safe_div(a: float, b: float) -> float:\n",
    "    return float(a / b) if b else 0.0\n",
    "\n",
    "\n",
    "def threshold_scan(y_true: pd.Series, probs: np.ndarray, thresholds: np.ndarray) -> pd.DataFrame:\n",
    "    y = y_true.to_numpy(dtype=int)\n",
    "    rows = []\n",
    "    for t in thresholds:\n",
    "        pred = (probs >= t).astype(int)\n",
    "        rows.append(\n",
    "            {\n",
    "                \"threshold\": float(t),\n",
    "                \"precision\": precision_score(y, pred, zero_division=0),\n",
    "                \"recall\": recall_score(y, pred, zero_division=0),\n",
    "                \"f1\": f1_score(y, pred, zero_division=0),\n",
    "                \"false_negatives\": int(((y == 1) & (pred == 0)).sum()),\n",
    "                \"false_positives\": int(((y == 0) & (pred == 1)).sum()),\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows).sort_values(\"threshold\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "def pick_best_threshold(thr_df: pd.DataFrame, min_recall: float) -> tuple[float, str]:\n",
    "    # Regola principale: massimizza precisione mantenendo recall minimo\n",
    "    feasible = thr_df[thr_df[\"recall\"] >= min_recall]\n",
    "    if not feasible.empty:\n",
    "        row = feasible.sort_values([\"precision\", \"f1\"], ascending=False).iloc[0]\n",
    "        return float(row[\"threshold\"]), f\"best_precision_with_recall>={min_recall:.2f}\"\n",
    "\n",
    "    # Fallback: se nessuna soglia raggiunge recall minimo, usa best F1\n",
    "    row = thr_df.sort_values([\"f1\", \"precision\"], ascending=False).iloc[0]\n",
    "    return float(row[\"threshold\"]), \"fallback_best_f1\"\n",
    "\n",
    "\n",
    "def evaluate_split(y_true: pd.Series, probs: np.ndarray, threshold: float) -> dict:\n",
    "    y = y_true.to_numpy(dtype=int)\n",
    "    pred = (probs >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "    return {\n",
    "        \"precision\": precision_score(y, pred, zero_division=0),\n",
    "        \"recall\": recall_score(y, pred, zero_division=0),\n",
    "        \"f1\": f1_score(y, pred, zero_division=0),\n",
    "        \"pr_auc\": average_precision_score(y, probs),\n",
    "        \"roc_auc\": roc_auc_score(y, probs),\n",
    "        \"tn\": int(tn),\n",
    "        \"fp\": int(fp),\n",
    "        \"fn\": int(fn),\n",
    "        \"tp\": int(tp),\n",
    "    }\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 3) LOAD DATA\n",
    "# ======================================================\n",
    "\n",
    "for p in [TRAIN_PATH, VAL_PATH, TEST_PATH]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing required file: {p}\")\n",
    "\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "val = pd.read_csv(VAL_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "X_train = train.drop(columns=[TARGET])\n",
    "y_train = train[TARGET].astype(int)\n",
    "\n",
    "X_val = val.drop(columns=[TARGET])\n",
    "y_val = val[TARGET].astype(int)\n",
    "\n",
    "X_test = test.drop(columns=[TARGET])\n",
    "y_test = test[TARGET].astype(int)\n",
    "\n",
    "features_order = list(X_train.columns)\n",
    "if list(X_val.columns) != features_order or list(X_test.columns) != features_order:\n",
    "    raise ValueError(\"Feature mismatch between train/val/test\")\n",
    "\n",
    "neg = int((y_train == 0).sum())\n",
    "pos = int((y_train == 1).sum())\n",
    "scale_pos_weight = safe_div(neg, pos)\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"positive rate:\", y_train.mean())\n",
    "print(\"Val  :\", X_val.shape, \"positive rate:\", y_val.mean())\n",
    "print(\"Test :\", X_test.shape, \"positive rate:\", y_test.mean())\n",
    "print(\"scale_pos_weight:\", scale_pos_weight)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 4) TRAIN BEST MODEL\n",
    "# ======================================================\n",
    "\n",
    "model = LGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1,\n",
    "    **LGBM_PARAMS,\n",
    ")\n",
    "\n",
    "print(\"\\nTraining tuned weighted LightGBM...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "val_probs = model.predict_proba(X_val)[:, 1]\n",
    "test_probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 5) THRESHOLD OPTIMIZATION ON VALIDATION\n",
    "# ======================================================\n",
    "\n",
    "thr_df = threshold_scan(y_val, val_probs, THRESHOLDS)\n",
    "best_thr, reason = pick_best_threshold(thr_df, MIN_RECALL)\n",
    "\n",
    "val_metrics = evaluate_split(y_val, val_probs, best_thr)\n",
    "test_metrics = evaluate_split(y_test, test_probs, best_thr)\n",
    "\n",
    "print(\"\\nSelected threshold:\", best_thr)\n",
    "print(\"Selection rule:\", reason)\n",
    "print(\"\\nValidation metrics:\", val_metrics)\n",
    "print(\"\\nTest metrics:\", test_metrics)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 6) FINAL TEST REPORT\n",
    "# ======================================================\n",
    "\n",
    "test_pred = (test_probs >= best_thr).astype(int)\n",
    "\n",
    "print(\"\\n===== CLASSIFICATION REPORT (TEST) =====\")\n",
    "print(classification_report(y_test, test_pred, digits=4))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, test_pred))\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 7) SAVE OUTPUTS\n",
    "# ======================================================\n",
    "\n",
    "# threshold scan table\n",
    "thr_path = REPORTS_DIR / \"test4_lgbm_threshold_scan.csv\"\n",
    "thr_df.to_csv(thr_path, index=False)\n",
    "\n",
    "# summary report\n",
    "summary = {\n",
    "    \"tactic\": \"tuned_weighted_lgbm\",\n",
    "    \"selection_rule\": reason,\n",
    "    \"threshold\": float(best_thr),\n",
    "    \"min_recall\": MIN_RECALL,\n",
    "    \"lgbm_params\": LGBM_PARAMS,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"val\": val_metrics,\n",
    "    \"test\": test_metrics,\n",
    "}\n",
    "\n",
    "summary_path = REPORTS_DIR / \"test4_lgbm_best_report.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "# false negatives for manual audit\n",
    "analysis = test.copy()\n",
    "analysis[\"prob\"] = test_probs\n",
    "analysis[\"pred\"] = test_pred\n",
    "fn = analysis[(analysis[TARGET] == 1) & (analysis[\"pred\"] == 0)]\n",
    "fn_path = ROOT / \"data\" / \"evaluation\" / \"false_negatives_test4_lgbm_best.csv\"\n",
    "fn.to_csv(fn_path, index=False)\n",
    "\n",
    "# save production bundle\n",
    "if SAVE_MODEL:\n",
    "    model_bundle = {\n",
    "        \"type\": \"lgbm_weighted_tuned\",\n",
    "        \"model\": model,\n",
    "        \"threshold\": float(best_thr),\n",
    "        \"features_order\": features_order,\n",
    "        \"metadata\": {\n",
    "            \"selection_rule\": reason,\n",
    "            \"min_recall\": MIN_RECALL,\n",
    "            \"scale_pos_weight\": scale_pos_weight,\n",
    "            \"lgbm_params\": LGBM_PARAMS,\n",
    "        },\n",
    "    }\n",
    "    model_out = ROOT / \"models\" / \"production\" / \"severe_model_test4_best_tactic.pkl\"\n",
    "    joblib.dump(model_bundle, model_out)\n",
    "    print(f\"\\nSaved model: {model_out}\")\n",
    "\n",
    "print(f\"Saved threshold table: {thr_path}\")\n",
    "print(f\"Saved summary report : {summary_path}\")\n",
    "print(f\"Saved FN file        : {fn_path}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/marcodonatiello/PycharmProjects/JupyterProject\n",
      "Train: (536370, 78) positive rate: 0.12842254413930682\n",
      "Val  : (134093, 78) positive rate: 0.12842579403846585\n",
      "Test : (167616, 78) positive rate: 0.12842449408171058\n",
      "scale_pos_weight: 6.786794808513109\n",
      "\n",
      "Training tuned weighted LightGBM...\n",
      "\n",
      "Selected threshold: 0.79\n",
      "Selection rule: best_precision_with_recall>=0.58\n",
      "\n",
      "Validation metrics: {'precision': 0.6616805411030177, 'recall': 0.5907903141513269, 'f1': 0.6242292235481793, 'pr_auc': 0.692871038576882, 'roc_auc': 0.9112179309750683, 'tn': 111670, 'fp': 5202, 'fn': 7047, 'tp': 10174}\n",
      "\n",
      "Test metrics: {'precision': 0.5851196856020007, 'recall': 0.6086592957353898, 'f1': 0.5966574069857462, 'pr_auc': 0.6472700170887296, 'roc_auc': 0.8920629566003202, 'tn': 136800, 'fp': 9290, 'fn': 8424, 'tp': 13102}\n",
      "\n",
      "===== CLASSIFICATION REPORT (TEST) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9420    0.9364    0.9392    146090\n",
      "           1     0.5851    0.6087    0.5967     21526\n",
      "\n",
      "    accuracy                         0.8943    167616\n",
      "   macro avg     0.7636    0.7725    0.7679    167616\n",
      "weighted avg     0.8962    0.8943    0.8952    167616\n",
      "\n",
      "Confusion matrix:\n",
      "[[136800   9290]\n",
      " [  8424  13102]]\n",
      "\n",
      "Saved model: /Users/marcodonatiello/PycharmProjects/JupyterProject/models/production/severe_model_test4_best_tactic.pkl\n",
      "Saved threshold table: /Users/marcodonatiello/PycharmProjects/JupyterProject/reports/metrics/test4_lgbm_threshold_scan.csv\n",
      "Saved summary report : /Users/marcodonatiello/PycharmProjects/JupyterProject/reports/metrics/test4_lgbm_best_report.json\n",
      "Saved FN file        : /Users/marcodonatiello/PycharmProjects/JupyterProject/data/evaluation/false_negatives_test4_lgbm_best.csv\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
