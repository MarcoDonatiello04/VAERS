{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T18:49:02.345541Z",
     "start_time": "2026-02-14T18:49:02.059563Z"
    }
   },
   "source": [
    "# ======================================================\n",
    "# TEST5 - THE AVENGERS ENSEMBLE (LITE)\n",
    "# Objective: Combine LGBM, XGB and MLP (CatBoost removed due to Py3.14 issue)\n",
    "# ======================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "# from catboost import CatBoostClassifier  <-- Removed\n",
    "\n",
    "# ======================================================\n",
    "# 1) CONFIG & HYPERPARAMETERS\n",
    "# ======================================================\n",
    "\n",
    "TARGET = \"IS_SEVERE\"\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Config loaded.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T18:49:03.739033Z",
     "start_time": "2026-02-14T18:49:02.347091Z"
    }
   },
   "source": [
    "def find_project_root() -> Path:\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for p in [cwd, *cwd.parents]:\n",
    "        if (p / \"data\").exists() and (p / \"models\").exists():\n",
    "            return p\n",
    "    return cwd\n",
    "\n",
    "ROOT = find_project_root()\n",
    "data_dir = ROOT / \"data\" / \"interim\" / \"splits\"\n",
    "\n",
    "train_path = data_dir / \"train_step6.csv\"\n",
    "val_path = data_dir / \"val_step6.csv\"\n",
    "test_path = data_dir / \"test_step6.csv\"\n",
    "\n",
    "print(f\"Loading data from {data_dir}...\")\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_val   = pd.read_csv(val_path)\n",
    "df_test  = pd.read_csv(test_path)\n",
    "\n",
    "X_train = df_train.drop(columns=[TARGET])\n",
    "y_train = df_train[TARGET]\n",
    "\n",
    "X_val   = df_val.drop(columns=[TARGET])\n",
    "y_val   = df_val[TARGET]\n",
    "\n",
    "X_test  = df_test.drop(columns=[TARGET])\n",
    "y_test  = df_test[TARGET]\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /Users/marcodonatiello/PycharmProjects/JupyterProject/data/interim/splits...\n",
      "Train: (536370, 78), Val: (134093, 78), Test: (167616, 78)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T18:49:04.229180Z",
     "start_time": "2026-02-14T18:49:03.791807Z"
    }
   },
   "source": [
    "# ======================================================\n",
    "# 2) MODEL DEFINITIONS\n",
    "# ======================================================\n",
    "\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "# A. LightGBM (Champion from Test 4)\n",
    "lgbm_params = {\n",
    "    \"n_estimators\": 650,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"num_leaves\": 95,\n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"reg_lambda\": 3.0,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"scale_pos_weight\": scale_pos_weight\n",
    "}\n",
    "\n",
    "# B. XGBoost (Diverse tree structure)\n",
    "xgb_params = {\n",
    "    \"n_estimators\": 500,\n",
    "    \"max_depth\": 8,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"eval_metric\": \"logloss\"\n",
    "}\n",
    "\n",
    "# D. MLP (Neural Network) - Needs Scaling!\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "mlp_params = {\n",
    "    \"hidden_layer_sizes\": (128, 64),\n",
    "    \"activation\": \"relu\",\n",
    "    \"solver\": \"adam\",\n",
    "    \"alpha\": 0.0001,\n",
    "    \"batch_size\": 256,\n",
    "    \"learning_rate_init\": 0.001,\n",
    "    \"max_iter\": 200,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"early_stopping\": True\n",
    "}\n",
    "\n",
    "print(\"Models configured (LGBM, XGB, MLP).\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models configured (LGBM, XGB, MLP).\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T18:49:41.697845Z",
     "start_time": "2026-02-14T18:49:04.230300Z"
    }
   },
   "source": [
    "# ======================================================\n",
    "# 3) TRAINING BASE LEARNERS\n",
    "# ======================================================\n",
    "\n",
    "models = {}\n",
    "preds_val = {}\n",
    "preds_test = {}\n",
    "\n",
    "# --- LightGBM ---\n",
    "print(\"Training LightGBM...\")\n",
    "lgbm = LGBMClassifier(**lgbm_params)\n",
    "lgbm.fit(X_train, y_train)\n",
    "preds_val['lgbm'] = lgbm.predict_proba(X_val)[:, 1]\n",
    "preds_test['lgbm'] = lgbm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# --- XGBoost ---\n",
    "print(\"Training XGBoost...\")\n",
    "xgb = XGBClassifier(**xgb_params)\n",
    "xgb.fit(X_train, y_train)\n",
    "preds_val['xgb'] = xgb.predict_proba(X_val)[:, 1]\n",
    "preds_test['xgb'] = xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# --- MLP ---\n",
    "print(\"Training MLP...\")\n",
    "mlp = MLPClassifier(**mlp_params)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "preds_val['mlp'] = mlp.predict_proba(X_val_scaled)[:, 1]\n",
    "preds_test['mlp'] = mlp.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"All models trained.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 68882, number of negative: 467488\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026999 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1042\n",
      "[LightGBM] [Info] Number of data points in the train set: 536370, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.128423 -> initscore=-1.914979\n",
      "[LightGBM] [Info] Start training from score -1.914979\n",
      "Training XGBoost...\n",
      "Training MLP...\n",
      "All models trained.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T18:49:41.792717Z",
     "start_time": "2026-02-14T18:49:41.715209Z"
    }
   },
   "source": [
    "# ======================================================\n",
    "# 4) ENSEMBLE OPTIMIZATION\n",
    "# ======================================================\n",
    "\n",
    "print(\"Optimizing Ensemble Weights...\")\n",
    "\n",
    "model_names = ['lgbm', 'xgb', 'mlp']\n",
    "val_matrix = np.column_stack([preds_val[m] for m in model_names])\n",
    "test_matrix = np.column_stack([preds_test[m] for m in model_names])\n",
    "\n",
    "def loss_func(weights):\n",
    "    # Normalize weights\n",
    "    w = weights / np.sum(weights)\n",
    "    final_prob = np.dot(val_matrix, w)\n",
    "    # Maximize ROC-AUC\n",
    "    return -roc_auc_score(y_val, final_prob)\n",
    "\n",
    "init_weights = np.ones(len(model_names)) / len(model_names)\n",
    "bounds = [(0, 1)] * len(model_names)\n",
    "cons = ({'type': 'eq', 'fun': lambda w: 1 - np.sum(w)})\n",
    "\n",
    "res = minimize(loss_func, init_weights, bounds=bounds, constraints=cons, method='SLSQP')\n",
    "best_weights = res.x / np.sum(res.x)\n",
    "\n",
    "print(\"Best Weights (LGBM, XGB, MLP):\")\n",
    "print(best_weights)\n",
    "\n",
    "# Combine Predictions\n",
    "final_test_prob = np.dot(test_matrix, best_weights)\n",
    "final_val_prob = np.dot(val_matrix, best_weights)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing Ensemble Weights...\n",
      "Best Weights (LGBM, XGB, MLP):\n",
      "[0.33333333 0.33333333 0.33333333]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T18:49:43.201386Z",
     "start_time": "2026-02-14T18:49:41.801930Z"
    }
   },
   "source": [
    "# ======================================================\n",
    "# 5) THRESHOLD TUNING & EVALUATION\n",
    "# ======================================================\n",
    "\n",
    "THRESHOLDS = np.arange(0.1, 0.95, 0.01)\n",
    "best_thr = 0.5\n",
    "best_prec = 0\n",
    "best_f1 = 0\n",
    "target_recall = 0.58\n",
    "\n",
    "# Strategy: Maximize Precision s.t. Recall >= 0.58\n",
    "for t in THRESHOLDS:\n",
    "    pred = (final_val_prob >= t).astype(int)\n",
    "    rec = recall_score(y_val, pred, zero_division=0)\n",
    "    prec = precision_score(y_val, pred, zero_division=0)\n",
    "    f1 = f1_score(y_val, pred, zero_division=0)\n",
    "    \n",
    "    if rec >= target_recall:\n",
    "        if prec > best_prec:\n",
    "            best_prec = prec\n",
    "            best_f1 = f1\n",
    "            best_thr = t\n",
    "\n",
    "print(f\"Selected Threshold: {best_thr:.2f} (Val Precision: {best_prec:.4f}, Val F1: {best_f1:.4f})\")\n",
    "\n",
    "final_pred_test = (final_test_prob >= best_thr).astype(int)\n",
    "\n",
    "print(\"\\n===== CLASSIFICATION REPORT (TEST - ENSEMBLE) =====\")\n",
    "print(classification_report(y_test, final_pred_test, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, final_pred_test))\n",
    "\n",
    "# Individual Models Performance for Comparison\n",
    "print(\"\\n--- Single Models AUC ---\")\n",
    "for m in model_names:\n",
    "    auc = roc_auc_score(y_test, preds_test[m])\n",
    "    print(f\"{m.upper()}: {auc:.4f}\")\n",
    "    \n",
    "print(f\"ENSEMBLE: {roc_auc_score(y_test, final_test_prob):.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Threshold: 0.64 (Val Precision: 0.6675, Val F1: 0.6248)\n",
      "\n",
      "===== CLASSIFICATION REPORT (TEST - ENSEMBLE) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9417    0.9350    0.9384    146090\n",
      "           1     0.5794    0.6073    0.5930     21526\n",
      "\n",
      "    accuracy                         0.8930    167616\n",
      "   macro avg     0.7606    0.7712    0.7657    167616\n",
      "weighted avg     0.8952    0.8930    0.8940    167616\n",
      "\n",
      "Confusion Matrix:\n",
      "[[136600   9490]\n",
      " [  8453  13073]]\n",
      "\n",
      "--- Single Models AUC ---\n",
      "LGBM: 0.8921\n",
      "XGB: 0.8909\n",
      "MLP: 0.8806\n",
      "ENSEMBLE: 0.8913\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
