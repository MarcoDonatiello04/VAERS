{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-24T23:07:30.195861Z",
     "start_time": "2026-01-24T23:07:27.460626Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. CONFIGURAZIONE PERCORSI SICURA ---\n",
    "# Usiamo il percorso assoluto della cartella corrente per non sbagliare\n",
    "BASE_DIR = os.getcwd() # Cartella dove stai eseguendo lo script\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, '../dataset')\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"üìÅ Cartella creata: {OUTPUT_DIR}\")\n",
    "\n",
    "# Cerchiamo il file di input\n",
    "POSSIBILI_PERCORSI = [\n",
    "    os.path.join(OUTPUT_DIR, 'VAERS_DEEP_CLEANED.csv'),\n",
    "    os.path.join(BASE_DIR, 'VAERS_DEEP_CLEANED.csv'),\n",
    "    'data/VAERS_DEEP_CLEANED.csv'\n",
    "]\n",
    "\n",
    "INPUT_FILE = None\n",
    "for percorso in POSSIBILI_PERCORSI:\n",
    "    if os.path.exists(percorso):\n",
    "        INPUT_FILE = percorso\n",
    "        print(f\"‚úÖ File input trovato in: {INPUT_FILE}\")\n",
    "        break\n",
    "\n",
    "if INPUT_FILE is None:\n",
    "    print(\"‚ùå ERRORE CRITICO: File input non trovato.\")\n",
    "    print(f\"Ho cercato in: {POSSIBILI_PERCORSI}\")\n",
    "    raise FileNotFoundError(\"VAERS_DEEP_CLEANED.csv non trovato\")\n",
    "\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_DIR, 'VAERS_SANITY_CHECKED.csv')\n",
    "\n",
    "print(f\"Caricamento dati...\")\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_FILE, low_memory=False)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Errore nella lettura del file: {e}\")\n",
    "    raise\n",
    "\n",
    "righe_iniziali = len(df)\n",
    "print(f\"Righe iniziali: {righe_iniziali}\")\n",
    "\n",
    "# --- 2. APPLICAZIONE REGOLE ---\n",
    "print(\"\\n--- INIZIO CONTROLLI DI COERENZA ---\")\n",
    "\n",
    "# ET√Ä\n",
    "mask_age = (df['AGE_YRS'] >= 0) & (df['AGE_YRS'] <= 110)\n",
    "df = df[mask_age]\n",
    "print(f\"1. Et√† filtrata. Righe attuali: {len(df)}\")\n",
    "\n",
    "# GIORNI (Ho corretto il limite a 1000 come nel tuo codice)\n",
    "if 'NUMDAYS' in df.columns:\n",
    "    mask_days = (df['NUMDAYS'] >= 0) & (df['NUMDAYS'] <= 1000)\n",
    "    df = df[mask_days]\n",
    "    print(f\"2. Giorni filtrati. Righe attuali: {len(df)}\")\n",
    "\n",
    "# SESSO\n",
    "df = df[df['SEX'].isin(['M', 'F'])]\n",
    "print(f\"3. Sesso filtrato. Righe attuali: {len(df)}\")\n",
    "\n",
    "# DOSE\n",
    "df = df[df['VAX_DOSE_SERIES'] >= 1]\n",
    "print(f\"4. Dose filtrata. Righe attuali: {len(df)}\")\n",
    "\n",
    "# TARGET\n",
    "df = df[df['IS_SEVERE'].isin([0, 1])]\n",
    "print(f\"5. Target filtrato. Righe attuali: {len(df)}\")\n",
    "\n",
    "# --- 3. SALVATAGGIO ---\n",
    "righe_finali = len(df)\n",
    "perse = righe_iniziali - righe_finali\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"SANITY CHECK COMPLETATO.\")\n",
    "print(f\"Righe eliminate: {perse}\")\n",
    "print(f\"Righe valide rimaste: {righe_finali}\")\n",
    "\n",
    "try:\n",
    "    df.to_csv(OUTPUT_FILE, index=False)\n",
    "    # Verifica immediata che il file esista\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        print(f\"‚úÖ SUCCESSO! File salvato in:\\n   üëâ {OUTPUT_FILE}\")\n",
    "        print(f\"   Dimensione file: {os.path.getsize(OUTPUT_FILE) / 1024:.2f} KB\")\n",
    "    else:\n",
    "        print(\"‚ùå ERRORE: Il comando ha finito ma il file non c'√®. Controlla i permessi!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERRORE DURANTE IL SALVATAGGIO: {e}\")\n",
    "    print(\"Suggerimento: Chiudi il file se √® aperto in Excel.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File input trovato in: /Users/marcodonatiello/PycharmProjects/JupyterProject/data/VAERS_DEEP_CLEANED.csv\n",
      "Caricamento dati...\n",
      "Righe iniziali: 838912\n",
      "\n",
      "--- INIZIO CONTROLLI DI COERENZA ---\n",
      "1. Et√† filtrata. Righe attuali: 838904\n",
      "2. Giorni filtrati. Righe attuali: 838083\n",
      "3. Sesso filtrato. Righe attuali: 838083\n",
      "4. Dose filtrata. Righe attuali: 838083\n",
      "5. Target filtrato. Righe attuali: 838083\n",
      "------------------------------\n",
      "SANITY CHECK COMPLETATO.\n",
      "Righe eliminate: 829\n",
      "Righe valide rimaste: 838083\n",
      "‚úÖ SUCCESSO! File salvato in:\n",
      "   üëâ /Users/marcodonatiello/PycharmProjects/JupyterProject/data/VAERS_SANITY_CHECKED.csv\n",
      "   Dimensione file: 154053.90 KB\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T23:40:51.748470Z",
     "start_time": "2026-01-24T23:40:48.337796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. CONFIGURAZIONE ---\n",
    "BASE_DIR = os.getcwd()\n",
    "# Input: Il Train Set che abbiamo creato nello step precedente\n",
    "INPUT_FILE = os.path.join(BASE_DIR, '../dataset', 'train.csv')\n",
    "\n",
    "OUTPUT_TRAIN_FINAL = os.path.join(BASE_DIR, '../dataset', 'train_final.csv')\n",
    "OUTPUT_VAL = os.path.join(BASE_DIR, '../dataset', 'validation.csv')\n",
    "\n",
    "print(f\"Caricamento Train Set Originale: {INPUT_FILE}\")\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    raise FileNotFoundError(\"ERRORE: Non trovo 'train.csv'. Hai fatto lo split precedente?\")\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE, low_memory=False)\n",
    "\n",
    "# --- 2. RICREAZIONE STRATIFICAZIONE (Bio-Logica) ---\n",
    "# Dobbiamo ricreare la colonna STRATA perch√© nel file salvato non c'√® pi√π.\n",
    "# Vogliamo che anche il Validation sia rappresentativo per et√†/sesso/gravit√†.\n",
    "\n",
    "bins = [-1, 18, 64, 150]\n",
    "labels = ['Young', 'Adult', 'Senior']\n",
    "df['AGE_BIN'] = pd.cut(df['AGE_YRS'], bins=bins, labels=labels)\n",
    "\n",
    "df['STRATA'] = (\n",
    "    df['IS_SEVERE'].astype(str) + \"_\" +\n",
    "    df['SEX'] + \"_\" +\n",
    "    df['AGE_BIN'].astype(str)\n",
    ")\n",
    "\n",
    "# Gestione gruppi rari (come nello script precedente)\n",
    "counts = df['STRATA'].value_counts()\n",
    "rare_groups = counts[counts < 2].index\n",
    "df.loc[df['STRATA'].isin(rare_groups), 'STRATA'] = df.loc[df['STRATA'].isin(rare_groups), 'IS_SEVERE'].astype(str)\n",
    "\n",
    "# --- 3. ESECUZIONE SPLIT TRAIN / VALIDATION ---\n",
    "# Solitamente si usa il 20% del Train come Validation (che equivale al 16% del totale originale)\n",
    "VAL_SIZE = 0.20\n",
    "\n",
    "print(f\"\\nGenerazione Validation Set ({VAL_SIZE*100}% del Train)...\")\n",
    "\n",
    "X = df.drop(columns=['IS_SEVERE'])\n",
    "y = df['IS_SEVERE']\n",
    "\n",
    "X_train_part, X_val, y_train_part, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=VAL_SIZE,\n",
    "    random_state=42,\n",
    "    stratify=df['STRATA'] # Stratificazione coerente\n",
    ")\n",
    "\n",
    "# --- 4. SALVATAGGIO ---\n",
    "# Riuniamo i pezzi\n",
    "train_final = X_train_part.copy()\n",
    "train_final['IS_SEVERE'] = y_train_part\n",
    "\n",
    "val_final = X_val.copy()\n",
    "val_final['IS_SEVERE'] = y_val\n",
    "\n",
    "# Pulizia colonne tecniche\n",
    "cols_to_drop = ['STRATA', 'AGE_BIN']\n",
    "train_final = train_final.drop(columns=cols_to_drop)\n",
    "val_final = val_final.drop(columns=cols_to_drop)\n",
    "\n",
    "train_final.to_csv(OUTPUT_TRAIN_FINAL, index=False)\n",
    "val_final.to_csv(OUTPUT_VAL, index=False)\n",
    "\n",
    "# --- 5. REPORT RIASSUNTIVO ---\n",
    "print(\"-\" * 40)\n",
    "print(f\"‚úÖ SUDDIVISIONE COMPLETATA\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"1. Train Set FINALE (Studio):    {len(train_final)} righe\")\n",
    "print(f\"   -> Salvato in: {OUTPUT_TRAIN_FINAL}\")\n",
    "print(f\"2. Validation Set (Controllo):   {len(val_final)} righe\")\n",
    "print(f\"   -> Salvato in: {OUTPUT_VAL}\")\n",
    "print(\"-\" * 40)\n",
    "print(\"üìä Percentuali di Gravit√† (Controllo Coerenza):\")\n",
    "perc_t = train_final['IS_SEVERE'].mean() * 100\n",
    "perc_v = val_final['IS_SEVERE'].mean() * 100\n",
    "print(f\"   - % Gravi nel Train Final: {perc_t:.2f}%\")\n",
    "print(f\"   - % Gravi nel Validation:  {perc_v:.2f}%\")"
   ],
   "id": "79a3721a897fe1b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento Train Set Originale: /Users/marcodonatiello/PycharmProjects/JupyterProject/data/train.csv\n",
      "\n",
      "Generazione Validation Set (20.0% del Train)...\n",
      "----------------------------------------\n",
      "‚úÖ SUDDIVISIONE COMPLETATA\n",
      "----------------------------------------\n",
      "1. Train Set FINALE (Studio):    536370 righe\n",
      "   -> Salvato in: /Users/marcodonatiello/PycharmProjects/JupyterProject/data/train_final.csv\n",
      "2. Validation Set (Controllo):   134093 righe\n",
      "   -> Salvato in: /Users/marcodonatiello/PycharmProjects/JupyterProject/data/validation.csv\n",
      "----------------------------------------\n",
      "üìä Percentuali di Gravit√† (Controllo Coerenza):\n",
      "   - % Gravi nel Train Final: 12.84%\n",
      "   - % Gravi nel Validation:  12.84%\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
